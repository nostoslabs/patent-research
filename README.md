# Neural Patent Similarity Search Research

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> Comprehensive research on neural embedding models for patent similarity search, featuring production-ready implementations and scientific analysis.

## ğŸ¯ Project Overview

This repository contains a complete research pipeline for evaluating and implementing neural patent similarity search systems. The project evaluates 4 embedding models across 1,625+ patents and provides production-ready implementations with sub-4 second query processing.

### Key Achievements
- **36Ã— speedup** with dedicated rerankers vs. LLM approaches
- **Production-ready system** processing 329 patents/minute  
- **Scientific validation** with proper statistical analysis
- **Novel insights** into embedding-LLM similarity disconnect (r=0.275)

---

## ğŸ“ Project Structure

```
patent_research/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ pyproject.toml              # Python dependencies
â”œâ”€â”€ .python-version             # Python version specification
â”œâ”€â”€ 
â”œâ”€â”€ code/                       # Implementation code
â”‚   â”œâ”€â”€ llm_provider_factory.py      # PydanticAI multi-provider LLM system
â”‚   â”œâ”€â”€ reranker_enhancement_plan.py # Advanced multi-reranker system  
â”‚   â”œâ”€â”€ cross_encoder_reranker.py    # Two-stage retrieval implementation
â”‚   â”œâ”€â”€ ground_truth_generator.py    # LLM-based similarity evaluation
â”‚   â”œâ”€â”€ google_patents_baseline.py   # Classification-based search baseline
â”‚   â”œâ”€â”€ scientific_analysis.py       # Statistical analysis & visualization
â”‚   â”œâ”€â”€ run_multimodel_experiments.py # Batch experiment runner
â”‚   â”œâ”€â”€ model_performance_analyzer.py # Performance analysis & ranking
â”‚   â”œâ”€â”€ comprehensive_evaluation.py   # End-to-end evaluation pipeline
â”‚   â””â”€â”€ download_large_diverse_patents.py # Dataset preparation
â”‚
â”œâ”€â”€ data/                       # Datasets and results
â”‚   â”œâ”€â”€ patent_abstracts.jsonl           # Base patent dataset
â”‚   â”œâ”€â”€ patent_abstracts_10k_diverse.jsonl # 10K diverse patents
â”‚   â”œâ”€â”€ patent_abstracts_100k_diverse.jsonl # 100K diverse patents  
â”‚   â”œâ”€â”€ patent_ground_truth_100.jsonl     # LLM-evaluated pairs
â”‚   â”œâ”€â”€ baseline_comparison_100_queries.jsonl # Baseline study results
â”‚   â”œâ”€â”€ *_embeddings.jsonl               # Generated embeddings
â”‚   â”œâ”€â”€ *_batch_results.json             # Experiment results
â”‚   â””â”€â”€ patent_similarity_statistical_analysis.json # Statistics
â”‚
â”œâ”€â”€ reports/                    # Research reports and analysis
â”‚   â”œâ”€â”€ SCIENTIFIC_PATENT_SEARCH_ANALYSIS.md  # ğŸ“„ Main scientific paper
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md                    # Complete project overview
â”‚   â”œâ”€â”€ MODEL_PERFORMANCE_ANALYSIS.md         # Detailed model comparison
â”‚   â”œâ”€â”€ RERANKER_PERFORMANCE_ANALYSIS.md      # Reranker evaluation
â”‚   â”œâ”€â”€ MODEL_COMPARISON_SUMMARY.md           # Executive summary
â”‚   â””â”€â”€ CHUNKING_EXPERIMENT_GUIDE.md          # Chunking analysis
â”‚
â”œâ”€â”€ visualizations/             # Generated plots and charts
â”‚   â”œâ”€â”€ patent_similarity_analysis.png   # 4-panel scientific visualization
â”‚   â”œâ”€â”€ correlation_heatmap.png          # Correlation matrix
â”‚   â””â”€â”€ chunking_analysis_visualizations/ # Chunking experiment plots
â”‚
â””â”€â”€ analysis/                   # Additional analysis files
    â””â”€â”€ [temporary analysis files]
```

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **uv** package manager (recommended) or pip
- **Ollama** with embedding models installed
- **API Keys** (optional): OpenAI, Google, Anthropic, Cohere

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd patent_research

# Install dependencies with uv (recommended)
uv sync

# OR install with pip
pip install -r requirements.txt

# Install Ollama embedding models
ollama pull nomic-embed-text
ollama pull bge-m3
ollama pull embeddinggemma  
ollama pull mxbai-embed-large
```

### Environment Setup

Create a `.env` file with your API keys (optional):
```env
OPENAI_API_KEY=your_openai_key_here
GOOGLE_API_KEY=your_google_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
COHERE_API_KEY=your_cohere_key_here
```

---

## ğŸ”¬ Usage Examples

### 1. Quick Patent Similarity Search

```python
from code.cross_encoder_reranker import EmbeddingSearchEngine

# Load embeddings
searcher = EmbeddingSearchEngine("data/patent_abstracts_with_embeddings.jsonl")

# Find similar patents
results = searcher.search_similar("patent_12345", top_k=10)
for result in results:
    print(f"{result.patent_id}: {result.similarity:.3f}")
```

### 2. Advanced Reranking

```python
from code.reranker_enhancement_plan import AdvancedRerankerSystem

# Initialize with BGE reranker
system = AdvancedRerankerSystem(
    embeddings_file="data/patent_abstracts_with_embeddings.jsonl",
    reranker_type="bge-reranker-base"
)

# Get initial results and rerank
initial_results = system.embedding_searcher.search_similar("patent_12345", 20)
reranked = await system.rerank_results(query_patent, initial_results, 10)
```

### 3. Run Complete Evaluation

```bash
# Generate embeddings for all models
uv run python code/run_multimodel_experiments.py compare data/patent_abstracts.jsonl

# Create ground truth dataset
uv run python code/ground_truth_generator.py data/patent_abstracts_with_embeddings.jsonl --pairs 100

# Run baseline comparison  
uv run python code/google_patents_baseline.py data/patent_abstracts_with_embeddings.jsonl --sample 100

# Generate scientific analysis
uv run python code/scientific_analysis.py
```

---

## ğŸ“Š Key Research Findings

### Embedding Model Rankings

| Model | Speed (pat/min) | Overall Score | Context Window | Production Ready |
|-------|-----------------|---------------|----------------|------------------|
| **nomic-embed-text** | 329 | 81.5/100 | 8,192 tokens | âœ… **Yes** |
| **bge-m3** | 133 | 66.0/100 | 8,192 tokens | âœ… Yes |
| **embeddinggemma** | 99 | 27.2/100 | 2,048 tokens | âš ï¸ Limited |
| **mxbai-embed-large** | 50 | 24.8/100 | 512 tokens | âŒ No |

### Reranker Performance

- **BGE-reranker-base**: 36Ã— faster than LLM reranking (3.3s vs 120s per query)
- **Production throughput**: 18 queries/minute with high-quality results
- **Recommendation**: Use BGE for speed, LLM fallback for explainability

### Statistical Validation

- **Sample size**: 50 patent pairs with structured LLM evaluation
- **Correlation**: r=0.275 (weak) between embedding and LLM similarity
- **Baseline overlap**: 15.2% between embedding and classification search
- **Significance**: Spearman Ï=0.358, p=0.011 (statistically significant)

---

## ğŸ—ï¸ Production Architecture

### Recommended Configuration

```python
# Production-ready two-stage system
EMBEDDING_MODEL = "nomic-embed-text"    # 329 patents/min
RERANKER_MODEL = "bge-reranker-base"    # 18 queries/min  
FALLBACK_LLM = "gemini-1.5-flash"      # For quality-critical queries

# Performance targets achieved:
# - Sub-4 second end-to-end processing
# - Real-time search on 10K+ patent corpora  
# - Cost-efficient local inference
```

### System Performance

- **Processing**: 10,000 patents in ~30 minutes (embedding generation)
- **Query speed**: <4 seconds end-to-end with reranking
- **Throughput**: ~1,000 queries/hour production capacity
- **Memory**: Minimal chunking (5% of patents) with 8K context models

---

## ğŸ”¬ Reproducing Research

### Complete Research Pipeline

```bash
# 1. Download and prepare datasets
uv run python code/download_large_diverse_patents.py 10k
uv run python code/download_large_diverse_patents.py 100k

# 2. Generate embeddings for all models
uv run python code/run_multimodel_experiments.py compare patent_abstracts_10k_diverse.jsonl --batch-name research

# 3. Create ground truth evaluation
uv run python code/ground_truth_generator.py data/patent_abstracts_with_embeddings.jsonl --pairs 200

# 4. Run baseline comparisons
uv run python code/google_patents_baseline.py data/patent_abstracts_with_embeddings.jsonl --sample 100

# 5. Test reranker systems
uv run python code/reranker_enhancement_plan.py data/patent_abstracts_with_embeddings.jsonl --benchmark

# 6. Generate scientific analysis
uv run python code/scientific_analysis.py
uv run python code/model_performance_analyzer.py

# 7. Create comprehensive evaluation
uv run python code/comprehensive_evaluation.py
```

### Expected Outputs

- **Embeddings**: Generated for all 4 models across datasets
- **Performance metrics**: Speed, accuracy, context efficiency  
- **Statistical analysis**: Correlations, distributions, significance tests
- **Visualizations**: Scientific plots and correlation heatmaps
- **Reports**: Publication-ready analysis documents

---

## ğŸ“š Key Documentation

### Scientific Papers
- **[SCIENTIFIC_PATENT_SEARCH_ANALYSIS.md](reports/SCIENTIFIC_PATENT_SEARCH_ANALYSIS.md)**: Main research paper with methodology, results, and analysis
- **[PROJECT_SUMMARY.md](reports/PROJECT_SUMMARY.md)**: Complete project overview and achievements

### Technical Documentation  
- **[MODEL_PERFORMANCE_ANALYSIS.md](reports/MODEL_PERFORMANCE_ANALYSIS.md)**: Detailed embedding model comparison
- **[RERANKER_PERFORMANCE_ANALYSIS.md](reports/RERANKER_PERFORMANCE_ANALYSIS.md)**: Reranker evaluation results

### Implementation Guides
- **[code/llm_provider_factory.py](code/llm_provider_factory.py)**: Multi-provider LLM integration
- **[code/reranker_enhancement_plan.py](code/reranker_enhancement_plan.py)**: Advanced reranking implementation

---

## ğŸ› ï¸ Development & Extension

### Adding New Embedding Models

1. **Install model in Ollama**: `ollama pull model-name`
2. **Add to experiment runner**: Edit `code/run_multimodel_experiments.py`
3. **Update analysis**: Add model to `code/model_performance_analyzer.py`

### Extending Rerankers

1. **Implement reranker**: Add to `code/reranker_enhancement_plan.py`  
2. **Add configuration**: Update `RERANKER_CONFIGS` dictionary
3. **Test performance**: Run benchmark comparison

### Custom Datasets

```python
# Prepare your patent data in JSONL format:
{"id": "patent_id", "abstract": "patent abstract text", "classification": "patent_class"}

# Then run the complete pipeline:
uv run python code/run_multimodel_experiments.py compare your_dataset.jsonl
```

---

## ğŸ¤ Contributing

### Research Contributions Needed

1. **Expand ground truth**: Increase sample size to nâ‰¥200 for robust statistics
2. **Human validation**: Compare LLM judgments against patent experts  
3. **Full document analysis**: Include claims, figures, and specifications
4. **Domain specialization**: Fine-tune models for specific technical areas

### Code Contributions

1. **Fork the repository**
2. **Create feature branch**: `git checkout -b feature/new-feature`
3. **Make changes** and test thoroughly
4. **Submit pull request** with detailed description

### Standards

- **Python 3.8+** compatibility
- **Type hints** for all functions
- **Comprehensive docstrings**
- **Unit tests** for new functionality
- **Scientific rigor** in analysis

---

## ğŸ“Š Performance Benchmarks

### System Requirements

- **CPU**: Multi-core recommended (embedding generation)
- **Memory**: 8GB+ RAM for large datasets  
- **GPU**: Optional (can accelerate embedding models)
- **Storage**: 10GB+ for complete datasets and results

### Scaling Considerations

- **10K patents**: Real-time search, <1GB storage
- **100K patents**: Near real-time, ~5GB storage  
- **1M+ patents**: Requires optimization, distributed processing

---

## ğŸ“ License & Citation

### License
This project is licensed under the MIT License - see LICENSE file for details.

### Citation
If you use this research in your work, please cite:

```bibtex
@misc{patent_similarity_research_2025,
  title={Neural Patent Similarity Search: A Comprehensive Evaluation of Embedding Models and Reranking Approaches},
  author={[Author Names]},
  year={2025},
  note={Available at: [Repository URL]}
}
```

---

## ğŸš¨ Limitations & Future Work

### Current Limitations

1. **Ground truth sample size**: n=50 pairs insufficient for robust correlation analysis
2. **Single LLM evaluator**: Results dependent on Gemini-1.5-Flash capabilities
3. **Abstract-only analysis**: Missing claims, figures, and technical specifications  
4. **Limited domain coverage**: 7 classification groups may not represent full diversity

### Immediate Next Steps

1. **Expand ground truth to nâ‰¥200** with stratified sampling
2. **Multi-evaluator validation** (human experts + multiple LLMs)  
3. **Full patent document analysis** incorporating all sections
4. **Cross-domain validation** across different technical fields

### Research Extensions

1. **Hybrid search systems** combining embedding + classification + graph approaches
2. **Explainable similarity** for patent professional workflows
3. **Temporal analysis** of patent similarity evolution  
4. **Multi-modal integration** with figures and chemical structures

---

## ğŸ“ Support & Contact

### Getting Help

1. **Check documentation** in `reports/` directory
2. **Review code comments** for implementation details
3. **Run example scripts** to understand usage patterns
4. **Open GitHub issues** for bugs or feature requests

### Research Collaboration

We welcome collaborations on:
- **Patent similarity research**
- **LLM evaluation methodologies**  
- **Production deployment case studies**
- **Domain-specific applications**

---

**ğŸ“ This repository provides a complete foundation for neural patent similarity research with production-ready implementations, scientific validation, and extensible architecture for future development.**