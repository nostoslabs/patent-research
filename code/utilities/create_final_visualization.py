#!/usr/bin/env python3
"""Create final visualization of accurate token rates."""

def create_performance_chart():
    """Create comprehensive performance visualization."""

    # Data from accurate testing
    llm_data = {
        'smollm2:latest': {'tokens_per_sec': 34.20, 'warm_up': 2.77},
        'llama3.2:latest': {'tokens_per_sec': 24.18, 'warm_up': 3.72},
        'gemma3:latest': {'tokens_per_sec': 12.25, 'warm_up': 8.28},
        'llama3.1:8b': {'tokens_per_sec': 5.74, 'warm_up': 9.66}
    }

    embedding_data = {
        'nomic-embed-text:latest': {'emb_per_sec': 46.33, 'warm_up': 6.70, 'dims': 768},
        'mxbai-embed-large:latest': {'emb_per_sec': 29.82, 'warm_up': 7.10, 'dims': 1024},
        'embeddinggemma:latest': {'emb_per_sec': 8.00, 'warm_up': 13.42, 'dims': 768},
        'bge-m3:latest': {'emb_per_sec': 6.63, 'warm_up': 2.32, 'dims': 1024}
    }

    print("OLLAMA ACCURATE TOKEN RATE ANALYSIS")
    print("=" * 50)
    print("âœ… Results after proper model warm-up")
    print()

    # LLM Performance Chart
    print("ğŸš€ LLM TOKEN GENERATION RATES")
    print("=" * 35)
    print()

    max_tokens = max(data['tokens_per_sec'] for data in llm_data.values())

    for i, (model, data) in enumerate(sorted(llm_data.items(), key=lambda x: x[1]['tokens_per_sec'], reverse=True), 1):
        bar_length = int((data['tokens_per_sec'] / max_tokens) * 40)
        bar = "â–ˆ" * bar_length
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f" {i}"

        print(f"{medal} {model:<25}")
        print(f"   {bar:<40} {data['tokens_per_sec']:>6.1f} tok/s")
        print(f"   Warm-up: {data['warm_up']:.1f}s")
        print()

    # Embedding Performance Chart
    print("âš¡ EMBEDDING GENERATION RATES")
    print("=" * 35)
    print()

    max_emb = max(data['emb_per_sec'] for data in embedding_data.values())

    for i, (model, data) in enumerate(sorted(embedding_data.items(), key=lambda x: x[1]['emb_per_sec'], reverse=True), 1):
        bar_length = int((data['emb_per_sec'] / max_emb) * 40)
        bar = "â–ˆ" * bar_length
        medal = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f" {i}"

        print(f"{medal} {model:<25}")
        print(f"   {bar:<40} {data['emb_per_sec']:>6.1f} emb/s")
        print(f"   Dimensions: {data['dims']}, Warm-up: {data['warm_up']:.1f}s")
        print()

    # Performance Comparison
    print("ğŸ“Š PERFORMANCE COMPARISON")
    print("=" * 30)
    print()

    print("Speed Tiers:")
    print("ğŸ”¥ Ultra-Fast:  30+ tokens/sec  â†’ smollm2")
    print("âš¡ Fast:        20+ tokens/sec  â†’ llama3.2")
    print("ğŸŸ¢ Good:        10+ tokens/sec  â†’ gemma3")
    print("ğŸŸ¡ Moderate:    5+ tokens/sec   â†’ llama3.1:8b")
    print()

    print("Warm-up Speed:")
    print("ğŸš€ Instant:     <5 seconds      â†’ smollm2, llama3.2")
    print("â±ï¸  Quick:       5-10 seconds    â†’ gemma3, llama3.1")
    print("â³ Moderate:    10+ seconds     â†’ Large models")
    print()

    # Recommendations
    print("ğŸ¯ RECOMMENDATIONS")
    print("=" * 20)
    print()
    print("ğŸ† Best Overall:     smollm2:latest")
    print("   â€¢ 34.20 tokens/sec after 2.77s warm-up")
    print("   â€¢ Perfect for real-time applications")
    print()
    print("âš–ï¸  Best Balance:     llama3.2:latest")
    print("   â€¢ 24.18 tokens/sec after 3.72s warm-up")
    print("   â€¢ Excellent speed + quality trade-off")
    print()
    print("ğŸ“ Best Quality:     gemma3:latest")
    print("   â€¢ 12.25 tokens/sec, detailed responses")
    print("   â€¢ Choose when quality > speed")
    print()
    print("ğŸ” Best Embedding:   nomic-embed-text:latest")
    print("   â€¢ 46.33 embeddings/sec, 768 dimensions")
    print("   â€¢ Fastest embedding processing")
    print()

    # Technical insights
    print("ğŸ”¬ TECHNICAL INSIGHTS")
    print("=" * 22)
    print()
    print("â€¢ Model warm-up is CRITICAL - 5-10x performance difference")
    print("â€¢ Smaller models (1-3B) often outperform larger ones in speed")
    print("â€¢ Embedding models are much faster than expected after warm-up")
    print("â€¢ gpt-oss models have loading issues requiring extended timeouts")
    print("â€¢ Token rates are highly consistent after warm-up period")

def main():
    create_performance_chart()

    print("\n" + "=" * 60)
    print("ğŸ“ COMPLETE BENCHMARK RESULTS AVAILABLE:")
    print("   â€¢ comprehensive_token_rate_report.md")
    print("   â€¢ quick_accurate_results.json")
    print("   â€¢ All benchmark scripts for reproduction")
    print("=" * 60)

if __name__ == "__main__":
    main()